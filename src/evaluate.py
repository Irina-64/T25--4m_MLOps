import pandas as pd
import numpy as np
import json
import joblib
import mlflow
import mlflow.sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    roc_auc_score,
    precision_score,
    recall_score,
    f1_score,
    accuracy_score,
    confusion_matrix,
    classification_report,
    roc_curve,
    auc
)
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os

def evaluate_model(model_path: str = None, log_to_mlflow: bool = True):
    """
    –û—Ü–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ –∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç.
    
    Args:
        model_path: –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ None, –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏–∑ models/)
        log_to_mlflow: –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –ª–∏ –º–µ—Ç—Ä–∏–∫–∏ –≤ MLflow
    
    Returns:
        dict: —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –æ—Ü–µ–Ω–∫–∏
    """
    
    print("=" * 80)
    print("–û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò")
    print("=" * 80)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤
    os.makedirs('reports', exist_ok=True)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    df = pd.read_csv('data/processed/processed.csv')
    print(f"‚úì –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –†–∞–∑–º–µ—Ä: {df.shape}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ features –∏ target
    X = df.drop('Churn', axis=1)
    y = df['Churn']
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"‚úì Train size: {X_train.shape}, Test size: {X_test.shape}")
    print(f"‚úì –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –≤ —Ç–µ—Å—Ç–µ: {y_test.value_counts().to_dict()}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    print("\nü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    if model_path is None:
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ø–æ—Å–ª–µ–¥–Ω—é—é –º–æ–¥–µ–ª—å
        model_files = [
            f for f in os.listdir('models/') 
            if f.endswith('.joblib')
        ]
        if not model_files:
            raise FileNotFoundError("–ù–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –ø–∞–ø–∫–µ models/")
        model_path = f"models/{sorted(model_files)[-1]}"
    
    model = joblib.load(model_path)
    print(f"‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑: {model_path}")
    print(f"  –¢–∏–ø –º–æ–¥–µ–ª–∏: {type(model).__name__}")
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    print("\nüîÆ –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
    y_pred = model.predict(X_test)
    
    # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è ROC-AUC
    if hasattr(model, 'predict_proba'):
        y_pred_proba = model.predict_proba(X_test)[:, 1]
    else:
        y_pred_proba = y_pred
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    print("\nüìà –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫...")
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    accuracy = accuracy_score(y_test, y_pred)
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    
    # Classification Report
    class_report = classification_report(y_test, y_pred, output_dict=True)
    
    # ROC Curve data
    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    metrics = {
        "timestamp": datetime.now().isoformat(),
        "model_path": model_path,
        "model_type": type(model).__name__,
        "test_size": int(X_test.shape[0]),
        "n_features": int(X_test.shape[1]),
        "metrics": {
            "roc_auc": float(roc_auc),
            "accuracy": float(accuracy),
            "precision": float(precision),
            "recall": float(recall),
            "f1": float(f1),
            "specificity": float(specificity)
        },
        "confusion_matrix": {
            "true_negatives": int(tn),
            "false_positives": int(fp),
            "false_negatives": int(fn),
            "true_positives": int(tp)
        },
        "class_distribution": {
            "class_0": int((y_test == 0).sum()),
            "class_1": int((y_test == 1).sum())
        },
        "classification_report": class_report
    }
    
    # –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫ –≤ –∫–æ–Ω—Å–æ–ª—å
    print("\n" + "="*80)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò")
    print("="*80)
    print(f"\nüìä –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:")
    print(f"  ‚Ä¢ ROC-AUC:    {roc_auc:.4f}")
    print(f"  ‚Ä¢ Accuracy:   {accuracy:.4f}")
    print(f"  ‚Ä¢ Precision:  {precision:.4f}")
    print(f"  ‚Ä¢ Recall:     {recall:.4f}")
    print(f"  ‚Ä¢ F1-Score:   {f1:.4f}")
    print(f"  ‚Ä¢ Specificity: {specificity:.4f}")
    
    print(f"\nüéØ Confusion Matrix:")
    print(f"  True Negatives:  {tn:4d}")
    print(f"  False Positives: {fp:4d}")
    print(f"  False Negatives: {fn:4d}")
    print(f"  True Positives:  {tp:4d}")
    
    print(f"\nüìã Classification Report:")
    print(classification_report(y_test, y_pred))
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –≤ JSON
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤...")
    json_path = 'reports/eval.json'
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, ensure_ascii=False, indent=2)
    print(f"‚úì JSON –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {json_path}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ HTML –æ—Ç—á–µ—Ç–∞ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π
    html_path = 'reports/eval.html'
    generate_html_report(
        html_path, metrics, X_test, y_test, y_pred, y_pred_proba
    )
    print(f"‚úì HTML –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {html_path}")
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow (–µ—Å–ª–∏ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π run)
    if log_to_mlflow:
        try:
            print("\nüì§ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow...")
            # –ï—Å–ª–∏ –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ run, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
            if mlflow.active_run() is None:
                mlflow.start_run(run_name=f"evaluation_{datetime.now().strftime('%H%M%S')}")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
            mlflow.log_metric("roc_auc", roc_auc)
            mlflow.log_metric("accuracy", accuracy)
            mlflow.log_metric("precision", precision)
            mlflow.log_metric("recall", recall)
            mlflow.log_metric("f1", f1)
            mlflow.log_metric("specificity", specificity)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
            mlflow.log_artifact(json_path)
            mlflow.log_artifact(html_path)
            
            print("‚úì –ú–µ—Ç—Ä–∏–∫–∏ –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω—ã –≤ MLflow")
        except Exception as e:
            print(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –≤ MLflow: {e}")
    
    print("\n" + "="*80)
    print("‚úÖ –û–¶–ï–ù–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
    print("="*80)
    
    return metrics


def generate_html_report(
    html_path: str,
    metrics: dict,
    X_test: pd.DataFrame,
    y_test: pd.Series,
    y_pred: np.ndarray,
    y_pred_proba: np.ndarray
):
    """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ HTML –æ—Ç—á–µ—Ç–∞ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π."""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('Evaluation Report - Model Performance', fontsize=16, fontweight='bold')
    
    # 1. Confusion Matrix
    cm = metrics['confusion_matrix']
    cm_array = np.array([
        [cm['true_negatives'], cm['false_positives']],
        [cm['false_negatives'], cm['true_positives']]
    ])
    sns.heatmap(
        cm_array,
        annot=True,
        fmt='d',
        cmap='Blues',
        ax=axes[0, 0],
        cbar=False
    )
    axes[0, 0].set_title('Confusion Matrix')
    axes[0, 0].set_ylabel('True Label')
    axes[0, 0].set_xlabel('Predicted Label')
    
    # 2. ROC Curve
    fpr, tpr, _ = __import__('sklearn.metrics', fromlist=['roc_curve']).roc_curve(
        y_test, y_pred_proba
    )
    roc_auc = metrics['metrics']['roc_auc']
    axes[0, 1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
    axes[0, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
    axes[0, 1].set_xlim([0.0, 1.0])
    axes[0, 1].set_ylim([0.0, 1.05])
    axes[0, 1].set_xlabel('False Positive Rate')
    axes[0, 1].set_ylabel('True Positive Rate')
    axes[0, 1].set_title('ROC Curve')
    axes[0, 1].legend(loc="lower right")
    
    # 3. Metrics Bar Chart
    metric_names = ['ROC-AUC', 'Accuracy', 'Precision', 'Recall', 'F1-Score']
    metric_values = [
        metrics['metrics']['roc_auc'],
        metrics['metrics']['accuracy'],
        metrics['metrics']['precision'],
        metrics['metrics']['recall'],
        metrics['metrics']['f1']
    ]
    colors = ['#1f77b4' if v >= 0.8 else '#ff7f0e' for v in metric_values]
    axes[1, 0].barh(metric_names, metric_values, color=colors)
    axes[1, 0].set_xlim([0, 1])
    axes[1, 0].set_xlabel('Score')
    axes[1, 0].set_title('Metrics Summary')
    for i, v in enumerate(metric_values):
        axes[1, 0].text(v + 0.02, i, f'{v:.4f}', va='center')
    
    # 4. Prediction Distribution
    axes[1, 1].hist(y_pred_proba[y_test == 0], bins=30, alpha=0.7, label='Class 0', color='blue')
    axes[1, 1].hist(y_pred_proba[y_test == 1], bins=30, alpha=0.7, label='Class 1', color='red')
    axes[1, 1].set_xlabel('Predicted Probability')
    axes[1, 1].set_ylabel('Frequency')
    axes[1, 1].set_title('Prediction Probability Distribution')
    axes[1, 1].legend()
    
    plt.tight_layout()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
    plot_path = html_path.replace('.html', '_plot.png')
    plt.savefig(plot_path, dpi=100, bbox_inches='tight')
    plt.close()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ HTML
    html_content = f"""
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Model Evaluation Report</title>
        <style>
            body {{
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                margin: 0;
                padding: 20px;
                background-color: #f5f5f5;
            }}
            .container {{
                max-width: 1200px;
                margin: 0 auto;
                background-color: white;
                padding: 30px;
                border-radius: 8px;
                box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
            }}
            h1 {{
                color: #333;
                border-bottom: 3px solid #007bff;
                padding-bottom: 10px;
            }}
            h2 {{
                color: #555;
                margin-top: 30px;
            }}
            .metrics-grid {{
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                gap: 15px;
                margin: 20px 0;
            }}
            .metric-card {{
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
                padding: 20px;
                border-radius: 8px;
                text-align: center;
            }}
            .metric-card.high {{
                background: linear-gradient(135deg, #84fab0 0%, #8fd3f4 100%);
                color: #333;
            }}
            .metric-card.low {{
                background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
                color: #333;
            }}
            .metric-value {{
                font-size: 32px;
                font-weight: bold;
                margin: 10px 0;
            }}
            .metric-label {{
                font-size: 14px;
                opacity: 0.9;
            }}
            table {{
                width: 100%;
                border-collapse: collapse;
                margin: 20px 0;
            }}
            th {{
                background-color: #007bff;
                color: white;
                padding: 12px;
                text-align: left;
            }}
            td {{
                padding: 10px 12px;
                border-bottom: 1px solid #ddd;
            }}
            tr:hover {{
                background-color: #f5f5f5;
            }}
            .plot-container {{
                text-align: center;
                margin: 30px 0;
            }}
            .plot-container img {{
                max-width: 100%;
                height: auto;
                border-radius: 8px;
            }}
            .info-box {{
                background-color: #e7f3ff;
                border-left: 4px solid #007bff;
                padding: 15px;
                margin: 15px 0;
                border-radius: 4px;
            }}
            .info-box strong {{
                color: #007bff;
            }}
            footer {{
                margin-top: 40px;
                padding-top: 20px;
                border-top: 1px solid #ddd;
                color: #666;
                font-size: 12px;
            }}
        </style>
    </head>
    <body>
        <div class="container">
            <h1>üéØ Model Evaluation Report</h1>
            
            <div class="info-box">
                <strong>Model Type:</strong> {metrics['model_type']}<br>
                <strong>Model Path:</strong> {metrics['model_path']}<br>
                <strong>Test Size:</strong> {metrics['test_size']} samples | 
                <strong>Features:</strong> {metrics['n_features']}<br>
                <strong>Generated:</strong> {metrics['timestamp']}
            </div>
            
            <h2>üìä Key Performance Metrics</h2>
            <div class="metrics-grid">
                <div class="metric-card {'high' if metrics['metrics']['roc_auc'] >= 0.8 else 'low'}">
                    <div class="metric-label">ROC-AUC</div>
                    <div class="metric-value">{metrics['metrics']['roc_auc']:.4f}</div>
                </div>
                <div class="metric-card {'high' if metrics['metrics']['accuracy'] >= 0.8 else 'low'}">
                    <div class="metric-label">Accuracy</div>
                    <div class="metric-value">{metrics['metrics']['accuracy']:.4f}</div>
                </div>
                <div class="metric-card {'high' if metrics['metrics']['precision'] >= 0.8 else 'low'}">
                    <div class="metric-label">Precision</div>
                    <div class="metric-value">{metrics['metrics']['precision']:.4f}</div>
                </div>
                <div class="metric-card {'high' if metrics['metrics']['recall'] >= 0.8 else 'low'}">
                    <div class="metric-label">Recall</div>
                    <div class="metric-value">{metrics['metrics']['recall']:.4f}</div>
                </div>
                <div class="metric-card {'high' if metrics['metrics']['f1'] >= 0.8 else 'low'}">
                    <div class="metric-label">F1-Score</div>
                    <div class="metric-value">{metrics['metrics']['f1']:.4f}</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">Specificity</div>
                    <div class="metric-value">{metrics['metrics']['specificity']:.4f}</div>
                </div>
            </div>
            
            <h2>üîç Confusion Matrix</h2>
            <table>
                <tr>
                    <th></th>
                    <th>Predicted Negative</th>
                    <th>Predicted Positive</th>
                </tr>
                <tr>
                    <th>Actual Negative</th>
                    <td>{metrics['confusion_matrix']['true_negatives']}</td>
                    <td>{metrics['confusion_matrix']['false_positives']}</td>
                </tr>
                <tr>
                    <th>Actual Positive</th>
                    <td>{metrics['confusion_matrix']['false_negatives']}</td>
                    <td>{metrics['confusion_matrix']['true_positives']}</td>
                </tr>
            </table>
            
            <h2>üìà Visualizations</h2>
            <div class="plot-container">
                <img src="{plot_path.split('/')[-1]}" alt="Evaluation Plots">
            </div>
            
            <h2>üìã Classification Report</h2>
            <table>
                <tr>
                    <th>Class</th>
                    <th>Precision</th>
                    <th>Recall</th>
                    <th>F1-Score</th>
                    <th>Support</th>
                </tr>
    """
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
    for class_name in ['0', '1']:
        class_data = metrics['classification_report'].get(class_name, {})
        html_content += f"""
                <tr>
                    <td><strong>Class {class_name}</strong></td>
                    <td>{class_data.get('precision', 0):.4f}</td>
                    <td>{class_data.get('recall', 0):.4f}</td>
                    <td>{class_data.get('f1-score', 0):.4f}</td>
                    <td>{int(class_data.get('support', 0))}</td>
                </tr>
        """
    
    html_content += """
            </table>
            
            <h2>üí° Key Insights</h2>
            <ul>
    """
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—ã–≤–æ–¥–æ–≤
    roc_auc = metrics['metrics']['roc_auc']
    accuracy = metrics['metrics']['accuracy']
    precision = metrics['metrics']['precision']
    recall = metrics['metrics']['recall']
    specificity = metrics['metrics']['specificity']
    
    insights = []
    if roc_auc >= 0.9:
        insights.append("‚úÖ Excellent ROC-AUC score - model has excellent discrimination ability")
    elif roc_auc >= 0.8:
        insights.append("‚úÖ Good ROC-AUC score - model shows good discrimination ability")
    else:
        insights.append("‚ö†Ô∏è Low ROC-AUC score - model discrimination ability needs improvement")
    
    if precision >= recall:
        insights.append(f"üìå Precision ({precision:.4f}) > Recall ({recall:.4f}) - model is conservative")
    else:
        insights.append(f"üìå Recall ({recall:.4f}) > Precision ({precision:.4f}) - model is aggressive")
    
    if specificity >= 0.8:
        insights.append(f"‚úÖ Good Specificity ({specificity:.4f}) - model correctly identifies negatives")
    else:
        insights.append(f"‚ö†Ô∏è Low Specificity ({specificity:.4f}) - many false positives")
    
    for insight in insights:
        html_content += f"                <li>{insight}</li>\n"
    
    html_content += """
            </ul>
            
            <footer>
                <p>Generated automatically by src/evaluate.py</p>
                <p>For more details, see eval.json</p>
            </footer>
        </div>
    </body>
    </html>
    """
    
    with open(html_path, 'w', encoding='utf-8') as f:
        f.write(html_content)


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏
    metrics = evaluate_model()
