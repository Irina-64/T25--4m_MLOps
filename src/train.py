import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import joblib
import mlflow
import mlflow.sklearn
import sys
import os
from datetime import datetime
import warnings
from feast import FeatureStore  # ‚Üê –î–û–ë–ê–í–¨–¢–ï –ò–ú–ü–û–†–¢ –ó–î–ï–°–¨!

warnings.filterwarnings('ignore')

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.preprocess import prepare_features, get_feature_names

def main():
    print("=" * 60)
    print("–õ–ê–ë–û–†–ê–¢–û–†–ù–ê–Ø 9: –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° FEAST FEATURE STORE")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–æ–¥–µ–ª–µ–π –µ—Å–ª–∏ –Ω–µ—Ç
    os.makedirs('models', exist_ok=True)

    # ==================== –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° FEAST ====================
    print("\n1. üì¶ –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø FEATURE STORE...")
    feast_success = False
    data = None

    try:
        # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Feast
        store = FeatureStore(repo_path="feature_repo/")
        print("   ‚úÖ Feature Store –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ entity_df —Å event_timestamp
        print("\n2. üîç –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –ò–ó FEAST...")
        
        # –ß–∏—Ç–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è record_id –∏ –¥–∞—Ç
        source_df = pd.read_parquet('feature_repo/data/currency_data.parquet')
        print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {source_df.shape[0]} —Å—Ç—Ä–æ–∫, {source_df.shape[1]} –∫–æ–ª–æ–Ω–æ–∫")
        
        # –°–æ–∑–¥–∞–µ–º entity_df —Å event_timestamp (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û!)
        entity_df = pd.DataFrame({
            'record_id': source_df['record_id'].tolist(),
            'event_timestamp': source_df['date'].tolist()
        })
        
        # 3. –ü–æ–ª—É—á–µ–Ω–∏–µ –í–°–ï–• –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ Feast
        print("\n3. üìä –ü–û–õ–£–ß–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í –ò–ó FEATURE STORE...")
        
        # –ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –≤–∞—à–µ–≥–æ definitions.py
        feature_list = [
            "currency_features:USD_RUB",
            "currency_features:EUR_RUB", 
            "currency_features:GBP_RUB",
            "currency_features:day_of_week",
            "currency_features:is_weekend",
            "currency_features:departure_hour_bucket",
            "currency_features:currency_pair",
            # –ï—Å–ª–∏ –µ—Å—Ç—å USD_RUB_target –≤ definitions.py, –¥–æ–±–∞–≤—å—Ç–µ:
            # "currency_features:USD_RUB_target",
        ]
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Feast
        feast_data = store.get_historical_features(
            entity_df=entity_df,
            features=feature_list
        ).to_df()
        
        print(f"   ‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(feast_data)} —Å—Ç—Ä–æ–∫ –∏–∑ Feature Store")
        print(f"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feast_data.columns)}")
        
        # 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        print("\n4. üîß –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø...")
        
        # –£–¥–∞–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ Feast
        cols_to_remove = ['event_timestamp', 'created_at']
        cols_to_remove = [c for c in cols_to_remove if c in feast_data.columns]
        
        if cols_to_remove:
            feast_data = feast_data.drop(cols_to_remove, axis=1)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        target_column = 'USD_RUB_target'
        
        if target_column in feast_data.columns:
            print(f"   ‚úÖ –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è '{target_column}' –µ—Å—Ç—å –≤ Feast")
            data = feast_data
        else:
            print(f"   ‚ö†Ô∏è  –¶–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π '{target_column}' –Ω–µ—Ç –≤ Feast")
            print("   –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ Feast —Å —Ü–µ–ª–µ–≤–æ–π –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –¥–ª—è target
            local_data = pd.read_csv('data/processed/processed.csv')
            local_data['date'] = pd.to_datetime(local_data['date'])
            local_data = prepare_features(local_data)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤
            if len(feast_data) == len(local_data):
                # –î–æ–±–∞–≤–ª—è–µ–º target –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                feast_data[target_column] = local_data[target_column].values
                data = feast_data
                print(f"   ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω '{target_column}' –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
            else:
                print(f"   ‚ùå –†–∞–∑–º–µ—Ä—ã –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç: Feast={len(feast_data)}, –õ–æ–∫–∞–ª—å–Ω—ã–µ={len(local_data)}")
                print("   –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏–∑ Feast...")
                
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –Ω–æ –ª–æ–≥–∏—Ä—É–µ–º Feast
                data = local_data
                feast_success = True  # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
                
        feast_success = True
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Feast: {e}")
        print("   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")


    # 5. Fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ Feast –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª
    if not feast_success or data is None:
        print("\n‚ö†Ô∏è  –ò–°–ü–û–õ–¨–ó–£–Æ–¢–°–Ø –õ–û–ö–ê–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï (fallback)...")
        data = pd.read_csv('data/processed/processed.csv')
        data['date'] = pd.to_datetime(data['date'])
        data = prepare_features(data)



    # ==================== –ü–û–î–ì–û–¢–û–í–ö–ê –ö –û–ë–£–ß–ï–ù–ò–Æ ====================
    print(f"\nüìä –ò–¢–û–ì–û–í–´–ô –ù–ê–ë–û–† –î–ê–ù–ù–´–•:")
    print(f"   –°—Ç—Ä–æ–∫: {len(data)}")
    print(f"   –ö–æ–ª–æ–Ω–æ–∫: {len(data.columns)}")
    print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {'FEAST' if feast_success else '–õ–æ–∫–∞–ª—å–Ω—ã–π'}")

    # === –î–û–ë–ê–í–¨–¢–ï –≠–¢–û–¢ –ë–õ–û–ö –î–õ–Ø –û–ß–ò–°–¢–ö–ò –î–ê–ù–ù–´–• ===
    print("\nüîç –û–ß–ò–°–¢–ö–ê –î–ê–ù–ù–´–• –û–¢ –ù–ï–ß–ò–°–õ–û–í–´–• –ö–û–õ–û–ù–û–ö...")

    # 1. –£–¥–∞–ª—è–µ–º –≤—Å–µ –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    numeric_cols = []
    non_numeric_cols = []

    for col in data.columns:
        if pd.api.types.is_numeric_dtype(data[col]):
            numeric_cols.append(col)
        else:
            non_numeric_cols.append(col)

    if non_numeric_cols:
        print(f"   –£–¥–∞–ª—è—é –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {non_numeric_cols}")
        data = data[numeric_cols]

    print(f"   –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(data.columns)} —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫")

    # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    target_column = 'USD_RUB_target'
    if target_column not in data.columns:


        print(f"‚ùå –û—à–∏–±–∫–∞: —Ü–µ–ª–µ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ {target_column} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {data.columns.tolist()}")
        return


    # 3. –ü–æ–ª—É—á–∞–µ–º –∏–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ, –∫—Ä–æ–º–µ target)
    feature_columns = [col for col in data.columns if col != target_column]

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —á–∏—Å–ª–æ–≤—ã–µ
    for col in feature_columns:
        if not pd.api.types.is_numeric_dtype(data[col]):
            print(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –∫–æ–ª–æ–Ω–∫–∞ {col} –Ω–µ —á–∏—Å–ª–æ–≤–∞—è ({data[col].dtype})")
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ
            try:
                data[col] = pd.to_numeric(data[col], errors='coerce')
                print(f"   –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∞ –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø")
            except:
                print(f"   –£–¥–∞–ª—è—é –∫–æ–ª–æ–Ω–∫—É {col}")
                feature_columns.remove(col)

    print(f"\nüìà –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø:")
    print(f"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_columns)}")
    print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {data[target_column].value_counts().to_dict()}")

    # –î–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
    tscv = TimeSeriesSplit(n_splits=5)


    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test —Å —É—á–µ—Ç–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
    train_size = int(0.8 * len(data))


    X_train = data[feature_columns].iloc[:train_size]
    X_test = data[feature_columns].iloc[train_size:]
    y_train = data[target_column].iloc[:train_size]
    y_test = data[target_column].iloc[train_size:]

    print(f"\nüîÄ –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–•:")
    print(f"   Train: {X_train.shape}")
    print(f"   Test:  {X_test.shape}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    print("\nüîç –ü–†–û–í–ï–†–ö–ê –¢–ò–ü–û–í –î–ê–ù–ù–´–• –ü–ï–†–ï–î –ú–ê–°–®–¢–ê–ë–ò–†–û–í–ê–ù–ò–ï–ú:")
    for i, col in enumerate(feature_columns[:5]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
        print(f"   {col}: {X_train[col].dtype}")

    # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
    print("\n‚öñÔ∏è  –ú–ê–°–®–¢–ê–ë–ò–†–û–í–ê–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í...")
    scaler = StandardScaler()


    try:
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        print("   ‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        
        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        print("   –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:")
        for col in feature_columns:
            print(f"     {col}: —Ç–∏–ø={X_train[col].dtype}, NaN={X_train[col].isna().sum()}")
        
        # –ü—Ä–æ–±—É–µ–º —É–¥–∞–ª–∏—Ç—å –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        print("   –ü—Ä–æ–±—É—é —É–¥–∞–ª–∏—Ç—å –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏...")
        problematic_cols = []
        for col in feature_columns:
            try:
                # –ü—Ä–æ–±—É–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∫ float
                test = X_train[col].astype(float)
            except:
                problematic_cols.append(col)
        
        if problematic_cols:
            print(f"   –£–¥–∞–ª—è—é –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {problematic_cols}")
            feature_columns = [c for c in feature_columns if c not in problematic_cols]
            X_train = data[feature_columns].iloc[:train_size]
            X_test = data[feature_columns].iloc[train_size:]
            
            # –ü–æ–≤—Ç–æ—Ä—è–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            print(f"   ‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è {len(problematic_cols)} –∫–æ–ª–æ–Ω–æ–∫")
        else:
            raise e

    # ==================== –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô ====================
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç MLflow
    mlflow.set_experiment("flight_delay")

    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
    models = {
        "RandomForest": RandomForestClassifier(
            n_estimators=100,
            max_depth=5,
            min_samples_split=20,
            min_samples_leaf=10,
            random_state=42
        ),
        "LogisticRegression": LogisticRegression(
            C=0.1,
            max_iter=1000,
            random_state=42
        )
    }

    best_score = 0
    best_model = None
    best_model_name = ""

    for model_name, model in models.items():

        print(f"\n--- –û–ë–£–ß–ï–ù–ò–ï {model_name} ---")

        with mlflow.start_run(run_name=model_name):
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
            if model_name == "RandomForest":
                mlflow.log_param("n_estimators", 100)
                mlflow.log_param("max_depth", 5)
                mlflow.log_param("min_samples_split", 20)
            else:  # LogisticRegression
                mlflow.log_param("C", 0.1)
                mlflow.log_param("max_iter", 1000)

            # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –¥–∞–Ω–Ω—ã—Ö
            mlflow.log_param("data_source", "feast" if feast_success else "local")
            mlflow.log_param("n_features", len(feature_columns))
            mlflow.log_param("feature_source", "feast+local_merge")
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            model.fit(X_train_scaled, y_train)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_pred = model.predict(X_test_scaled)
            y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            accuracy = accuracy_score(y_test, y_pred)
            roc_auc = roc_auc_score(y_test, y_pred_proba)


            # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ä—è–¥–∞–º–∏ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
            print("\nüîÑ –ö–†–û–°–°-–í–ê–õ–ò–î–ê–¶–ò–Ø...")
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º train –∏ test –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
            X_full = pd.concat([X_train, X_test])
            y_full = pd.concat([y_train, y_test])
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            X_full_scaled = scaler.transform(X_full)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é
            cv_scores = cross_val_score(model, X_full_scaled, y_full, cv=tscv, scoring='roc_auc')

            print(f"Accuracy: {accuracy:.4f}")
            print(f"ROC AUC: {roc_auc:.4f}")
            print(f"CV ROC AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")


            # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ MLflow
            mlflow.log_metric("accuracy", accuracy)
            mlflow.log_metric("roc_auc", roc_auc)
            mlflow.log_metric("cv_roc_auc_mean", cv_scores.mean())
            mlflow.log_metric("cv_roc_auc_std", cv_scores.std())


            # –õ–æ–≥–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –≤ MLflow
            mlflow.sklearn.log_model(
                sk_model=model,
                artifact_path="model",
                registered_model_name=f"{model_name}_flight_delay"
            )



            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –ª–æ–∫–∞–ª—å–Ω–æ
            model_filename = f"models/{model_name.lower()}_model.joblib"
            joblib.dump(model, model_filename)


            mlflow.log_artifact(model_filename, artifact_path="models")


            # –î–ª—è DVC
            if model_name == "RandomForest":
                joblib.dump(model, "models/random_forest_model.joblib")
                mlflow.log_artifact("models/random_forest_model.joblib", artifact_path="dvc_models")


            # –û–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            if roc_auc > best_score:
                best_score = roc_auc
                best_model = model
                best_model_name = model_name

    print(f"\n=== –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model_name} —Å ROC AUC: {best_score:.4f} ===")



    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
    if best_model is not None:
        joblib.dump(best_model, "models/best_model.joblib")
        joblib.dump(scaler, "models/scaler.joblib")
        joblib.dump(feature_columns, "models/feature_names.joblib")


        # –õ–æ–≥–∏—Ä—É–µ–º –≤ MLflow
        with mlflow.start_run(run_name="best_model"):
            mlflow.log_param("best_model", best_model_name)
            mlflow.log_param("data_source", "feast" if feast_success else "local")
            mlflow.log_metric("best_roc_auc", best_score)
            mlflow.sklearn.log_model(
                sk_model=best_model,
                artifact_path="best_model",
                registered_model_name="best_flight_delay_model"
            )


        print("‚úÖ –ú–æ–¥–µ–ª–∏ –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!")


        # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        y_pred_best = best_model.predict(X_test_scaled)


        print(f"\nüìä –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò:")
        print(f"Accuracy: {accuracy_score(y_test, y_pred_best):.4f}")


        print(f"Confusion Matrix:\n{confusion_matrix(y_test, y_pred_best)}")
        print(f"Classification Report:\n{classification_report(y_test, y_pred_best)}")

if __name__ == "__main__":
    main()