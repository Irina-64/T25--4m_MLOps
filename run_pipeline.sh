#!/bin/bash

# Telco Churn MLOps Pipeline - LAB 11 WITH DOCKER MONITORING
echo "======================================================================"
echo "üöÄ TELCO CHURN MLOPS PIPELINE"
echo "======================================================================"

# Colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Function to check status
check_status() {
    if [ $? -eq 0 ]; then
        echo -e "${GREEN}‚úì $1${NC}"
        return 0
    else
        echo -e "${RED}‚úó $1${NC}"
        return 1
    fi
}

print_header() {
    echo ""
    echo -e "${CYAN}‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê${NC}"
    echo -e "${BLUE}$1${NC}"
    echo -e "${CYAN}‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê${NC}"
}

# ============================================
print_header "‚úÖ –ü–†–û–í–ï–†–ö–ê –û–ö–†–£–ñ–ï–ù–ò–Ø –ò DOCKER"
# ============================================

# Check Python
if ! command -v python &> /dev/null; then
    echo -e "${RED}‚ùå Python –Ω–µ –Ω–∞–π–¥–µ–Ω${NC}"
    exit 1
fi
echo -e "${GREEN}‚úì Python –Ω–∞–π–¥–µ–Ω${NC}"

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Docker (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –¥–ª—è Lab 11)
echo "–ü—Ä–æ–≤–µ—Ä–∫–∞ Docker..."
if ! command -v docker &> /dev/null; then
    echo -e "${RED}‚ùå DOCKER –ù–ï –£–°–¢–ê–ù–û–í–õ–ï–ù!${NC}"
    echo "–î–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã 11 —Ç—Ä–µ–±—É–µ—Ç—Å—è Docker Desktop"
    echo "1. –°–∫–∞—á–∞–π—Ç–µ —Å: https://docs.docker.com/desktop/"
    echo "2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ Docker Desktop"
    echo "3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Ä–º–∏–Ω–∞–ª –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞"
    exit 1
fi

if ! docker info &> /dev/null; then
    echo -e "${RED}‚ùå DOCKER –ù–ï –ó–ê–ü–£–©–ï–ù!${NC}"
    echo "1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Docker Desktop"
    echo "2. –î–æ–∂–¥–∏—Ç–µ—Å—å –ø–æ–ª–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ (–∑–Ω–∞—á–æ–∫ –≤ —Ç—Ä–µ–µ —Å—Ç–∞–Ω–µ—Ç –±–µ–ª—ã–º/–∑–µ–ª–µ–Ω—ã–º)"
    echo "3. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞"
    echo ""
    echo "–ï—Å–ª–∏ Docker Desktop –∑–∞–ø—É—â–µ–Ω, –Ω–æ –≤—Å—ë —Ä–∞–≤–Ω–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç:"
    echo "  - Windows: –∑–∞–ø—É—Å—Ç–∏—Ç–µ PowerShell –∫–∞–∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä –∏ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:"
    echo "      wsl --update"
    echo "      wsl --shutdown"
    echo "      Restart-Service Docker*"
    exit 1
fi

echo -e "${GREEN}‚úÖ Docker –∑–∞–ø—É—â–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ${NC}"
echo "Docker –≤–µ—Ä—Å–∏—è: $(docker --version)"

# ============================================
print_header "üì¶ –£–°–¢–ê–ù–û–í–ö–ê –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô"
# ============================================

echo "–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π..."
pip install --quiet pandas numpy scikit-learn joblib mlflow matplotlib seaborn fastapi uvicorn pytest 2>/dev/null
check_status "–û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏"

echo "–£—Å—Ç–∞–Ω–æ–≤–∫–∞ MLOps –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π..."
pip install --quiet dvc feast prometheus-client 2>/dev/null
check_status "MLOps –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏"

# ============================================
print_header "üîÑ –®–ê–ì 1: –ü–†–ï–ü–†–û–¶–ï–°–°–ò–ù–ì –î–ê–ù–ù–´–•"
# ============================================

echo "–ó–∞–ø—É—Å–∫ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞..."
python src/preprocess.py
check_status "–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω"

# ============================================
print_header "ü§ñ –®–ê–ì 2: –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò"
# ============================================

echo "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏..."
python src/train.py 2>/dev/null || echo -e "${YELLOW}‚ö† –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ${NC}"
check_status "–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞"

# ============================================
print_header "üìä –®–ê–ì 3: –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò"
# ============================================

echo "–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏..."
if [ -f "src/evaluate.py" ]; then
    python src/evaluate.py --log-to-mlflow false 2>/dev/null || python src/evaluate.py
    check_status "–û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞"
else
    echo -e "${YELLOW}‚ö† evaluate.py –Ω–µ –Ω–∞–π–¥–µ–Ω${NC}"
fi

# ============================================
print_header "üê≥ –®–ê–ì 4: –ü–û–î–ì–û–¢–û–í–ö–ê DOCKER –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê"
# ============================================

echo "–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞..."

# –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
mkdir -p prometheus grafana/provisioning/datasources grafana/provisioning/dashboards grafana/dashboards

# 1. –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é Prometheus
cat > prometheus/prometheus.yml << 'EOF'
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'telco_churn_api'
    static_configs:
      - targets: ['host.docker.internal:8080']
        labels:
          service: 'telco-churn-api'
          environment: 'development'
  
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
EOF

echo "‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Prometheus —Å–æ–∑–¥–∞–Ω–∞"

# 2. –°–æ–∑–¥–∞–µ–º datasource –¥–ª—è Grafana
cat > grafana/provisioning/datasources/datasource.yml << 'EOF'
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
EOF

echo "‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Grafana datasource —Å–æ–∑–¥–∞–Ω–∞"

# 3. –°–æ–∑–¥–∞–µ–º Docker Compose –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
cat > docker-compose-monitoring.yml << 'EOF'
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    networks:
      - monitoring

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    networks:
      - monitoring
    depends_on:
      - prometheus

networks:
  monitoring:
    driver: bridge

volumes:
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
EOF

echo "‚úÖ Docker Compose –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞"

# ============================================
print_header "üöÄ –®–ê–ì 5: –ó–ê–ü–£–°–ö –ü–†–û–ú–ï–¢–ï–Ø –ò –ì–†–ê–§–ê–ù–´"
# ============================================

echo "–ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞..."
docker-compose -f docker-compose-monitoring.yml down 2>/dev/null
docker-compose -f docker-compose-monitoring.yml up -d

echo "–û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤..."
sleep 10

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–ø—É—Å–∫
echo "–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–ø—É—â–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤:"
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}" | grep -E "(prometheus|grafana)"

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
echo ""
echo "–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Prometheus..."
if curl -s http://localhost:9090 > /dev/null; then
    echo -e "${GREEN}‚úÖ Prometheus –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ http://localhost:9090${NC}"
else
    echo -e "${RED}‚ùå Prometheus –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω${NC}"
    echo "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏: docker logs prometheus"
fi

echo "–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Grafana..."
sleep 5
if curl -s http://localhost:3000 > /dev/null; then
    echo -e "${GREEN}‚úÖ Grafana –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ http://localhost:3000${NC}"
    echo "    –õ–æ–≥–∏–Ω: admin"
    echo "    –ü–∞—Ä–æ–ª—å: admin"
else
    echo -e "${RED}‚ùå Grafana –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω${NC}"
    echo "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏: docker logs grafana"
fi

# ============================================
print_header "üéØ –®–ê–ì 6: –ó–ê–ü–£–°–ö API –° –ú–ï–¢–†–ò–ö–ê–ú–ò PROMETHEUS"
# ============================================

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –æ–±–Ω–æ–≤–ª–µ–Ω –ª–∏ api.py —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
if ! grep -q "prometheus_client" src/api.py 2>/dev/null; then
    echo "–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ API –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –º–µ—Ç—Ä–∏–∫ Prometheus..."
    
    # –°–æ–∑–¥–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π api.py —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
    cat > src/api_prometheus.py << 'EOF'
from fastapi import FastAPI, HTTPException, Request
import joblib
import pandas as pd
import os
import time
from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST
from starlette.responses import Response
from starlette.middleware.base import BaseHTTPMiddleware

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ Prometheus
REQUEST_COUNT = Counter(
    'http_requests_total', 
    'Total HTTP Requests', 
    ['method', 'endpoint', 'status_code']
)

REQUEST_LATENCY = Histogram(
    'http_request_duration_seconds', 
    'HTTP request latency',
    ['method', 'endpoint']
)

PREDICTION_DISTRIBUTION = Histogram(
    'prediction_probability', 
    'Prediction probability distribution',
    buckets=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
)

MODEL_LOAD_COUNT = Counter(
    'model_load_total',
    'Total model load attempts',
    ['status']
)

# Middleware –¥–ª—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫ HTTP
class PrometheusMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        start_time = time.time()
        
        try:
            response = await call_next(request)
            status_code = response.status_code
        except Exception:
            status_code = 500
            raise
        finally:
            latency = time.time() - start_time
            REQUEST_LATENCY.labels(
                method=request.method,
                endpoint=request.url.path
            ).observe(latency)
            
            REQUEST_COUNT.labels(
                method=request.method,
                endpoint=request.url.path,
                status_code=status_code
            ).inc()
        
        return response

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(title="Telco Churn Prediction API with Prometheus", version="1.1.0")

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ middleware
app.add_middleware(PrometheusMiddleware)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
def _load_model():
    model_dir = "models"
    
    try:
        possible_paths = [
            os.path.join(model_dir, "model.joblib"),
            os.path.join(model_dir, "telco_churn_model.joblib"),
            os.path.join(model_dir, "logisticregression_model.joblib"),
            os.path.join(model_dir, "randomforest_model.joblib"),
        ]
        
        for model_path in possible_paths:
            if os.path.exists(model_path):
                print(f"üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å: {model_path}")
                MODEL_LOAD_COUNT.labels(status='success').inc()
                return joblib.load(model_path)
        
        if not os.path.isdir(model_dir):
            MODEL_LOAD_COUNT.labels(status='error').inc()
            raise FileNotFoundError(f"Model directory '{model_dir}' not found")
        
        candidates = [os.path.join(model_dir, f) for f in os.listdir(model_dir) if f.endswith('.joblib')]
        if not candidates:
            MODEL_LOAD_COUNT.labels(status='error').inc()
            raise FileNotFoundError(f"No .joblib models found in '{model_dir}'")
        
        latest = max(candidates, key=os.path.getmtime)
        print(f"üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –º–æ–¥–µ–ª—å: {latest}")
        MODEL_LOAD_COUNT.labels(status='success').inc()
        return joblib.load(latest)
        
    except Exception as e:
        MODEL_LOAD_COUNT.labels(status='error').inc()
        raise

try:
    model = _load_model()
    _MODEL_PATH = getattr(model, '_loaded_from', None)
except Exception as e:
    model = None
    _load_error = str(e)

# –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –º–µ—Ç—Ä–∏–∫ Prometheus
@app.get("/metrics")
def metrics():
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)

# –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤—å—è
@app.get("/health")
def health_check():
    return {
        "status": "healthy" if model is not None else "unhealthy",
        "model_loaded": model is not None,
        "timestamp": time.time(),
        "version": "1.1.0-prometheus"
    }

# –û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
@app.post("/predict")
def predict(payload: dict):
    """Prediction endpoint with Prometheus metrics."""
    if model is None:
        raise HTTPException(status_code=500, detail=f"Model not loaded: {_load_error}")

    try:
        df = pd.DataFrame([payload])
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Invalid payload: {e}")

    try:
        preds = model.predict_proba(df)[:, 1]
        prediction_prob = float(preds[0])
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        PREDICTION_DISTRIBUTION.observe(prediction_prob)
        
        return {"delay_prob": prediction_prob}
        
    except Exception as e:
        if hasattr(model, 'feature_names_in_'):
            cols = list(model.feature_names_in_)
            for c in cols:
                if c not in df.columns:
                    df[c] = 0
            df = df[cols]
            try:
                preds = model.predict_proba(df)[:, 1]
                prediction_prob = float(preds[0])
                PREDICTION_DISTRIBUTION.observe(prediction_prob)
                return {"delay_prob": prediction_prob}
            except Exception as e2:
                raise HTTPException(status_code=400, detail=f"Prediction failed after aligning features: {e2}")
        else:
            raise HTTPException(status_code=400, detail=f"Prediction failed: {e}")

@app.get("/")
def root():
    return {
        "message": "Telco Churn Prediction API with Prometheus Metrics",
        "version": "1.1.0",
        "monitoring": {
            "metrics": "GET /metrics",
            "health": "GET /health",
            "prometheus": "http://localhost:9090",
            "grafana": "http://localhost:3000 (admin/admin)"
        },
        "model_loaded": model is not None
    }
EOF
    
    # –î–µ–ª–∞–µ–º backup –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ api.py –∏ –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ –Ω–æ–≤—ã–π
    if [ -f "src/api.py" ]; then
        cp src/api.py src/api_backup.py
        echo "‚úÖ –°–æ–∑–¥–∞–Ω backup –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ api.py"
    fi
    mv src/api_prometheus.py src/api.py
    echo "‚úÖ API –æ–±–Ω–æ–≤–ª–µ–Ω –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ Prometheus –º–µ—Ç—Ä–∏–∫"
fi

echo "–ó–∞–ø—É—Å–∫ API —Å–µ—Ä–≤–µ—Ä–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–µ—Ç—Ä–∏–∫ Prometheus..."
pkill -f "uvicorn src.api:app" 2>/dev/null || true

# –ó–∞–ø—É—Å–∫–∞–µ–º API –≤ —Ñ–æ–Ω–µ
uvicorn src.api:app --host 0.0.0.0 --port 8080 --reload &
API_PID=$!
sleep 8

echo "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã API —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏..."
curl -s http://localhost:8080/health

echo ""
echo "–û—Ç–ø—Ä–∞–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–µ—Ç—Ä–∏–∫..."
curl -X POST "http://localhost:8080/predict" \
  -H "Content-Type: application/json" \
  -d '{"SeniorCitizen": 0, "tenure": 12, "MonthlyCharges": 50}' \
  --max-time 5

echo ""
check_status "API —Å Prometheus –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∑–∞–ø—É—â–µ–Ω"

# ============================================
print_header "üìà –®–ê–ì 7: –ù–ê–°–¢–†–û–ô–ö–ê –ì–†–ê–§–ê–ù–´ –ò –ì–ï–ù–ï–†–ê–¶–ò–Ø –ù–ê–ì–†–£–ó–ö–ò"
# ============================================

# –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –¥–∞—à–±–æ—Ä–¥ –¥–ª—è Grafana
cat > grafana/dashboards/telco_monitoring.json << 'EOF'
{
  "dashboard": {
    "title": "Telco Churn API Monitoring",
    "description": "Real-time monitoring of ML prediction API with Prometheus",
    "tags": ["mlops", "prometheus", "monitoring"],
    "style": "dark",
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "HTTP Requests Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[1m])",
            "legendFormat": "{{method}} {{endpoint}}",
            "refId": "A"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
        "description": "HTTP requests per second"
      },
      {
        "id": 2,
        "title": "Request Latency (95th percentile)",
        "type": "stat",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "format": "s",
            "instant": false,
            "refId": "A"
          }
        ],
        "gridPos": {"h": 4, "w": 6, "x": 0, "y": 8},
        "description": "95th percentile response time in seconds"
      },
      {
        "id": 3,
        "title": "Prediction Probability Distribution",
        "type": "heatmap",
        "targets": [
          {
            "expr": "rate(prediction_probability_bucket[5m])",
            "format": "heatmap",
            "refId": "A"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0},
        "description": "Distribution of prediction probabilities"
      },
      {
        "id": 4,
        "title": "Model Load Status",
        "type": "piechart",
        "targets": [
          {
            "expr": "model_load_total",
            "legendFormat": "{{status}}",
            "refId": "A"
          }
        ],
        "gridPos": {"h": 4, "w": 6, "x": 12, "y": 8},
        "pieType": "pie",
        "description": "Model load success/error count"
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "10s",
    "schemaVersion": 27,
    "version": 1
  },
  "folderId": 0,
  "overwrite": true
}
EOF

echo "‚úÖ –î–∞—à–±–æ—Ä–¥ Grafana —Å–æ–∑–¥–∞–Ω"

# –°–æ–∑–¥–∞–µ–º —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞–≥—Ä—É–∑–∫–∏
cat > generate_load.py << 'EOF'
import requests
import time
import random
import json
from concurrent.futures import ThreadPoolExecutor, as_completed

def generate_payload():
    """Generate random payload for testing."""
    return {
        "SeniorCitizen": random.randint(0, 1),
        "tenure": random.randint(1, 72),
        "MonthlyCharges": round(random.uniform(20, 120), 2),
        "TotalCharges": round(random.uniform(100, 8000), 2),
        "gender": random.randint(0, 1),
        "Partner": random.randint(0, 1),
        "Dependents": random.randint(0, 1)
    }

def send_request(request_id, url="http://localhost:8080/predict"):
    """Send single request to API."""
    try:
        start = time.time()
        response = requests.post(
            url,
            json=generate_payload(),
            timeout=10
        )
        latency = time.time() - start
        
        if response.status_code == 200:
            return {"id": request_id, "success": True, "latency": latency}
        else:
            return {"id": request_id, "success": False, "latency": latency}
            
    except Exception as e:
        return {"id": request_id, "success": False, "error": str(e)}

def run_load_test(num_requests=50, max_workers=5):
    """Run load test against API."""
    print(f"üöÄ Starting load test: {num_requests} requests with {max_workers} workers")
    print("=" * 50)
    
    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(send_request, i) for i in range(num_requests)]
        
        for future in as_completed(futures):
            results.append(future.result())
    
    # Analyze results
    successful = sum(1 for r in results if r.get('success', False))
    failed = num_requests - successful
    latencies = [r['latency'] for r in results if 'latency' in r]
    
    print(f"\nüìä Load Test Results:")
    print(f"   Total Requests: {num_requests}")
    print(f"   Successful: {successful} ({successful/num_requests*100:.1f}%)")
    print(f"   Failed: {failed} ({failed/num_requests*100:.1f}%)")
    
    if latencies:
        avg_latency = sum(latencies) / len(latencies) * 1000
        print(f"   Average Latency: {avg_latency:.1f}ms")
        print(f"   Requests per Second: {num_requests/sum(latencies):.2f}")
    
    print("=" * 50)
    return successful > 0

if __name__ == "__main__":
    # Warm up
    print("üî• Warming up API with 5 requests...")
    for _ in range(5):
        try:
            requests.post("http://localhost:8080/predict", 
                         json=generate_payload(), 
                         timeout=5)
        except:
            pass
    
    # Run actual test
    run_load_test(num_requests=60, max_workers=8)
    
    print("\n‚úÖ Load test completed!")
    print("üìà Check metrics at: http://localhost:8080/metrics")
    print("üìä Check Prometheus: http://localhost:9090")
    print("üé® Check Grafana: http://localhost:3000 (admin/admin)")
EOF

echo "–ó–∞–ø—É—Å–∫ –Ω–∞–≥—Ä—É–∑–æ—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–µ—Ç—Ä–∏–∫..."
python generate_load.py

# ============================================
print_header "üîç –®–ê–ì 8: –ü–†–û–í–ï–†–ö–ê –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê"
# ============================================

echo "–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞..."

# –°–æ–∑–¥–∞–µ–º —Å–∫—Ä–∏–ø—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏
cat > check_monitoring.py << 'EOF'
import requests
import time
import sys

def check_component(name, url, timeout=5):
    """Check if component is accessible."""
    try:
        response = requests.get(url, timeout=timeout)
        if response.status_code < 500:
            print(f"‚úÖ {name}: –¥–æ—Å—Ç—É–ø–µ–Ω ({url})")
            return True
        else:
            print(f"‚ùå {name}: –æ—à–∏–±–∫–∞ HTTP {response.status_code}")
            return False
    except Exception as e:
        print(f"‚ùå {name}: –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - {e}")
        return False

def check_prometheus_targets():
    """Check if Prometheus sees our API."""
    try:
        response = requests.get("http://localhost:9090/api/v1/targets", timeout=5)
        if response.status_code == 200:
            data = response.json()
            targets = data['data']['activeTargets']
            api_targets = [t for t in targets if '8080' in t['scrapeUrl']]
            
            if api_targets:
                print(f"‚úÖ Prometheus –≤–∏–¥–∏—Ç API target: {api_targets[0]['scrapeUrl']}")
                print(f"   –°–æ—Å—Ç–æ—è–Ω–∏–µ: {api_targets[0]['health']}")
                return True
            else:
                print("‚ö† Prometheus –Ω–µ –≤–∏–¥–∏—Ç API target")
                return False
    except Exception as e:
        print(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å Prometheus targets: {e}")
        return False

def check_metrics():
    """Check if metrics are being collected."""
    try:
        response = requests.get("http://localhost:8080/metrics", timeout=5)
        if response.status_code == 200:
            content = response.text
            metrics = [
                'http_requests_total',
                'http_request_duration_seconds',
                'prediction_probability',
                'model_load_total'
            ]
            
            print("üìà –ù–∞–π–¥–µ–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ Prometheus:")
            for metric in metrics:
                if metric in content:
                    print(f"   ‚úì {metric}")
                else:
                    print(f"   ‚ö† {metric} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            
            # Count requests
            lines = content.split('\n')
            request_lines = [l for l in lines if 'http_requests_total' in l and not l.startswith('#')]
            if request_lines:
                print(f"\nüìä –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {request_lines[0]}")
            
            return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –º–µ—Ç—Ä–∏–∫: {e}")
        return False

def main():
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ Lab 11")
    print("=" * 60)
    
    components = [
        ("API", "http://localhost:8080/health"),
        ("API Metrics", "http://localhost:8080/metrics"),
        ("Prometheus", "http://localhost:9090"),
        ("Grafana", "http://localhost:3000")
    ]
    
    all_ok = True
    for name, url in components:
        if not check_component(name, url):
            all_ok = False
    
    print("\n" + "=" * 60)
    
    if all_ok:
        print("‚úÖ –í–°–ï –ö–û–ú–ü–û–ù–ï–ù–¢–´ –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê –†–ê–ë–û–¢–ê–Æ–¢!")
        print("\nüìå –î–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã:")
        print("   1. API:              http://localhost:8080")
        print("   2. API –º–µ—Ç—Ä–∏–∫–∏:      http://localhost:8080/metrics")
        print("   3. Prometheus UI:    http://localhost:9090")
        print("   4. Grafana:          http://localhost:3000 (admin/admin)")
        print("   5. Grafana –∏–º–ø–æ—Ä—Ç –¥–∞—à–±–æ—Ä–¥–∞:")
        print("      - –ó–∞–ª–æ–≥–∏–Ω—å—Ç–µ—Å—å –≤ Grafana")
        print("      - –°–æ–∑–¥–∞–π—Ç–µ –Ω–æ–≤—ã–π –¥–∞—à–±–æ—Ä–¥")
        print("      - –î–æ–±–∞–≤—å—Ç–µ –ø–∞–Ω–µ–ª—å —Å –∑–∞–ø—Ä–æ—Å–æ–º Prometheus")
        print("      - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ—Ç—Ä–∏–∫–∏: http_requests_total, prediction_probability, etc.")
        return 0
    else:
        print("‚ö† –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Ç—Ä–µ–±—É—é—Ç –≤–Ω–∏–º–∞–Ω–∏—è")
        print("\nüîß –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—é:")
        print("   - –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Docker Desktop –∑–∞–ø—É—â–µ–Ω")
        print("   - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ: docker ps (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å prometheus –∏ grafana)")
        print("   - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏: docker logs prometheus")
        print("   - –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ: docker-compose -f docker-compose-monitoring.yml restart")
        return 1

if __name__ == "__main__":
    sys.exit(main())
EOF

echo "–ó–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞..."
python check_monitoring.py

# ============================================
print_header "‚úÖ –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢"
# ============================================

echo "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:"
echo "-------------------------------------------"

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
echo "1. Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã:"
docker ps --format "{{.Names}} {{.Status}} {{.Ports}}" | while read line; do
    echo "   ‚úÖ $line"
done

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ—Ç—Ä–∏–∫
echo "2. –ú–µ—Ç—Ä–∏–∫–∏ Prometheus:"
if curl -s http://localhost:8080/metrics | grep -q "http_requests_total"; then
    echo "   ‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ API —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è"
else
    echo "   ‚ùå –ú–µ—Ç—Ä–∏–∫–∏ –Ω–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è"
fi

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
echo "3. –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–∏—Å–æ–≤:"
services=(
    "API:8080"
    "Prometheus:9090" 
    "Grafana:3000"
)

for service in "${services[@]}"; do
    name=${service%:*}
    port=${service#*:}
    if curl -s "http://localhost:$port" > /dev/null 2>&1; then
        echo "   ‚úÖ $name –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É $port"
    else
        echo "   ‚ùå $name –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
    fi
done

# ============================================
echo ""
echo ""
echo -e "${CYAN}üìä –°–ò–°–¢–ï–ú–ê –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê –ó–ê–ü–£–©–ï–ù–ê:${NC}"
echo ""
echo "  1. FastAPI —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ Prometheus"
echo "     URL:      http://localhost:8080"
echo "     –ú–µ—Ç—Ä–∏–∫–∏:  http://localhost:8080/metrics"
echo "     Health:   http://localhost:8080/health"
echo ""
echo "  2. Prometheus (—Å–±–æ—Ä –º–µ—Ç—Ä–∏–∫)"
echo "     URL:      http://localhost:9090"
echo "     Targets:  http://localhost:9090/targets"
echo "     Graph:    http://localhost:9090/graph"
echo ""
echo "  3. Grafana (–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è)"
echo "     URL:      http://localhost:3000"
echo "     –õ–æ–≥–∏–Ω:    admin"
echo "     –ü–∞—Ä–æ–ª—å:   admin"
echo ""
echo -e "${CYAN}üß™ –ö–û–ú–ê–ù–î–´ –î–õ–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:${NC}"
echo ""
echo "  ‚Ä¢ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:"
echo "      python check_monitoring.py"
echo ""
echo "  ‚Ä¢ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–≥—Ä—É–∑–∫—É:"
echo "      python generate_load.py"
echo ""
echo "  ‚Ä¢ –°–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:"
echo '      curl -X POST "http://localhost:8080/predict" \'
echo '        -H "Content-Type: application/json" \'
echo '        -d '"'"'{"SeniorCitizen": 0, "tenure": 34, "MonthlyCharges": 56.95}'"'"''
echo ""
echo "  ‚Ä¢ –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –º–µ—Ç—Ä–∏–∫–∏:"
echo "      curl http://localhost:8080/metrics | grep http_requests_total"
echo ""
echo -e "${CYAN}üõë –ö–û–ú–ê–ù–î–´ –î–õ–Ø –û–°–¢–ê–ù–û–í–ö–ò:${NC}"
echo ""
echo "  ‚Ä¢ –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å API:"
echo "      pkill -f \"uvicorn src.api:app\""
echo ""
echo "  ‚Ä¢ –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:"
echo "      docker-compose -f docker-compose-monitoring.yml down"
echo ""
echo "  ‚Ä¢ –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤—Å—ë:"
echo "      pkill -f \"uvicorn\" && docker-compose -f docker-compose-monitoring.yml down"
echo ""
echo "======================================================================"
echo "üí° –î–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –º–µ—Ç—Ä–∏–∫ –≤ Grafana:"
echo "   1. –û—Ç–∫—Ä–æ–π—Ç–µ http://localhost:3000"
echo "   2. –í–æ–π–¥–∏—Ç–µ (admin/admin)"
echo "   3. –ù–∞–∂–º–∏—Ç–µ '+' ‚Üí Import dashboard"
echo "   4. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ JSON –∏–∑ grafana/dashboards/telco_monitoring.json"
echo "======================================================================"


# –û–∂–∏–¥–∞–Ω–∏–µ Ctrl+C –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
trap 'echo ""; echo "–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∏—Å—Ç–µ–º—É..."; kill $API_PID 2>/dev/null; docker-compose -f docker-compose-monitoring.yml down; rm -f .api_pid.lab11 generate_load.py check_monitoring.py; echo "‚úÖ –°–∏—Å—Ç–µ–º–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞"; exit' INT

echo ""
echo "‚ö†  –°–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç. –î–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–∞–∂–º–∏—Ç–µ Ctrl+C"
echo ""

# –ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª
while true; do
    sleep 1
done